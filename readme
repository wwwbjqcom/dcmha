背景:
    由于我们业务需求要实现多DC架构，在设计最初考虑过使用mha、双master架构，这些都能满足大部分业务需求，
    但在多机房的情况下管理起来会很繁琐一套业务一套系统，有多少个dc就直接翻多少倍，每套都要做检查，且mha
    在不修改源码的情况下不能做持续性的维护，切一次需要重启一次，服务端还存在单点的情况，而双master就必
    然引入VIP，这有可能会出现脑裂，还有在以前的维护中我们的双master+vip的架构出现过keepalived协议通，
    服务器tcp协议不通的情况，这导致了不能正常切换使我心有余悸，基于上面这些问题考虑，我们需要去除单点问
    题，维护性要简便且服务端要可持续性，还有需要能管理跨dc数据同步的问题，经过调研决定开发一套基于zk管
    理配置的高可用管理框架，就是这套系统，目前在我们系统正常跑着暂无问题

功能：
    1、前端使用haproxy做路由端，后端切换使业务层无感知
    2、通过zk同步配置文件，在路由端启动一个client实时监听路由变化
    3、master宕机切换可实现未同步数据追加及回滚，回滚的数据记录到本地目录已备人为判断是否需要
    4、因网络波动造成的心跳丢失会连续尝试数据库服务连接，如正常将不会发生切换并发出告警信息通知管理员
    5、服务端、路由端都可以集群化避免单点故障
    6、haproxy通过配置不同的集群组名称辨别，配置文件中通过read、write实现读写的端口分离
    7、检测slave是否在线，如不在线将从读列表中去除，后期将加入延迟检测，超过阈值剔除
		
操作实现：
    1、手动对集群进行维护时需先对该集群设置白名单（white）,操作完成后直接设置白名单任务（task）让server获取新master并监听
    2、如master监听丢失但重试连接mysql成功，server不会发生切换操作，会在watch-down节点写写入该集群信息，以供报警作用
    3、添加空集群直接使用add参数即可，自动选举master并同步
    4、宕机恢复的master如需恢复到集群中提供业务支持，执行my_mha中的setha以增加信息到路由端，如不恢复到集群中在下次master
       宕机切换时也可以自动作为活跃主机添加
    5、当前mysql有多个同步线程，需添加附加任务，在addition节点下先插入对应数据


不支持项：
    1、不支持多通道复制，只能以附加业务的方式指定
    2、不支持mysql的多点写入
    3、未平台化，操作基本使用my_mha.py脚本实现

注意事项：
    1、第一次启动需先启动server初始化zookeeper信息，再启动每个mysql端的心跳线程，最后执行my_mha相关项
    2、无论是手动修改、自动切换、初始化添加都会对路由信息进行同步，所以在初始化集群原始信息时务必先增加路由信息到zookeeper中,并把路由节点上的router.py启动
    3、mysqlrouter配置文件使用默认配置、集群名称命名的配置文件作为组合启动(groupname.conf)
        可以在单个节点上启动多个mysqlrouter，只需启动一个router.py，会自动判断组名并重启
    4、路由节点可以为多个以避免单节点故障，格式为host:port,host:port.....


介绍：
    zookeeper：
        white_path ： 手动维护集群时如不需要切换需设置的项，利用my_mha().setwhite设置
        lock_path  ： 在server多节点运行的模式下用于控制master宕机任务只能一个节点进行维护，创建节点名与任务名相同
        task_path  ： 记录所有任务数据，宕机任务以集群名称创建，其余手动设置的任务以task建立序列
        meta_host  ： 该节点下记录所有IP的元数据，格式['group':a,'port':123]
        meta_group ： 记录所有集群组信息，格式为(host,host....)
        meta_router： 记录每个集群路由所在地，以集群组名称命名内容格式为(host:port,host:port),port为router.py配置的端口
        online_path： 记录所有在线的节点
        master_path： 记录每个组当前的master所在，以集群组名称命名
        haproxy_path：记录路由配置文件，以机群组名称命名，记录格式{'read':[host:port,host:port...],'write':'host:port'},单节点写入多节点读取
        watch_down ： 记录两种情况，一种是因网络波动或客户端原因导致心跳丢失但mysql却正常的情况，以集群名称创建一个空节点，另一种为同步路由配置文件
                      失败的情况，以集群名称加send后缀命名
        addition   ： 节点下有replication、region两个节点
            replication/groupanme/regionname：如果集群组有附加的主从同步任务将在groupname创建对应节点，需要同步到那些位置的regionname会创建对应节点
                                              regionname下记录当前指向的IP信息，格式如{'host':'192.168.212.127','port':3316,'ssl':1/0}
            region/regionname: region节点下记录所有分区同步接口元数据，每个regionname下记录元数据，元数据可以为多个供高可用随机选择，记录格式如下
                               {'192-168-212-127':{'prot':3316,'ssl':1/0},'192-168-212-128':{}........}

部署流程：
    1、安装依赖 yum -y install epel-release gcc gcc-c++;yum -y install python-pip python-devel mysql-devel MySQL-python;pip install pymysql psutil kazoo
    2、部署zookeeper集群
    3、启动server端初始化数据
    4、在所有mysql节点部署dcmha_client分之创建心跳
    5、部署mysqlrouter并开启router.py打开端口
    6、执行my_mha中的各项
haproxy配置示例：
	global
	    maxconn 4096
	    daemon
	    pidfile /tmp/hkmsg.pid
	defaults
	    mode                tcp
	    timeout connect     20s
	    timeout client      1m
	    timeout server      1m
	listen write
	    bind :3598
	    mode tcp
	    balance roundrobin
	    server hkmsg 192.168.1.1:3306 check
	listen read
	    bind :3599
	    mode tcp
	    balance roundrobin
	    server hkmsg1 192.168.1.1:3306 check
	    server hkmsg2 192.168.1.2:3306 check
